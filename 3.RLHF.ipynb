{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLHF llama2 7b using SM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U sagemaker boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {}\n",
    "SM_TRAIN_DIR = \"/opt/ml/input/data\" \n",
    "\n",
    "hyperparameters[\"actor_model_name_or_path\"] = \"/opt/ml/input/data/sft\"\n",
    "hyperparameters[\"critic_model_name_or_path\"] = \"/opt/ml/input/data/reward\"\n",
    "hyperparameters[\"output_dir\"] =  \"/opt/ml/model\"\n",
    "hyperparameters[\"data_path\"] = \"Dahoas/rm-static\"\n",
    "hyperparameters[\"data_split\"] = \"2,4,4\"\n",
    "hyperparameters[\"num_padding_at_beginning\"] = 1\n",
    "hyperparameters[\"per_device_generation_batch_size\"] = 1\n",
    "hyperparameters[\"per_device_training_batch_size\"] = 1\n",
    "#hyperparameters[\"num_padding_at_beginning\"] = 0\n",
    "hyperparameters[\"generation_batches\"] = 1\n",
    "hyperparameters[\"ppo_epochs\"] = 1\n",
    "hyperparameters[\"max_answer_seq_len\"] = 256\n",
    "hyperparameters[\"max_prompt_seq_len\"] = 256\n",
    "hyperparameters[\"actor_learning_rate\"] = 9.65e-6\n",
    "hyperparameters[\"critic_learning_rate\"] = 5e-6\n",
    "hyperparameters[\"actor_weight_decay\"] = 0.1\n",
    "hyperparameters[\"critic_weight_decay\"] = 0.1\n",
    "hyperparameters[\"num_train_epochs\"] = 1\n",
    "hyperparameters[\"lr_scheduler_type\"] = \"cosine\"\n",
    "hyperparameters[\"gradient_accumulation_steps\"] = 1\n",
    "hyperparameters[\"actor_gradient_checkpointing\"] = \"\"\n",
    "\n",
    "hyperparameters[\"critic_gradient_checkpointing\"] = \"\"\n",
    "hyperparameters[\"offload_reference_model\"] = \"\"\n",
    "hyperparameters[\"disable_actor_dropout\"] = \"\"\n",
    "hyperparameters[\"deepspeed\"] = \"\"\n",
    "\n",
    "hyperparameters[\"actor_zero_stage\"] = 3\n",
    "hyperparameters[\"critic_zero_stage\"] = 3\n",
    "hyperparameters[\"enable_hybrid_engine\"] = \"\"\n",
    "\n",
    "hyperparameters[\"enable_mixed_precision_lora\"] = \"\"\n",
    "hyperparameters[\"actor_lora_dim\"] = 64\n",
    "hyperparameters[\"critic_lora_dim\"] = 64\n",
    "hyperparameters[\"critic_lora_module_name\"] = \"layers.\"\n",
    "hyperparameters[\"actor_lora_module_name\"] = \"layers.\"\n",
    "\n",
    "#hyperparameters[\"dtype\"] = \"bf16\"\n",
    "\n",
    "hyperparameters[\"output_dir\"] = \"/opt/ml/model\"\n",
    "\n",
    "\n",
    "hyperparameters[\"access_token\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = {}\n",
    "env['FI_PROVIDER'] = 'efa'\n",
    "env['NCCL_PROTO'] = 'simple'\n",
    "env['FI_EFA_USE_DEVICE_RDMA'] = '1'\n",
    "env['RDMAV_FORK_SAFE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_name = \"llama7b-rlhf-dschat\"\n",
    "estimator = PyTorch(\n",
    "    base_job_name=base_job_name,\n",
    "    source_dir=\"./scripts\",\n",
    "    entry_point=\"rlhf/main.py\",\n",
    "    role=role,\n",
    "    framework_version=\"1.13.1\",\n",
    "    py_version=\"py39\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.p4de.24xlarge\",\n",
    "    hyperparameters=hyperparameters,\n",
    "    disable_profiler=True,\n",
    "    environment=env,\n",
    "    distribution={\"torch_distributed\": {\"enabled\": True}},\n",
    "    keep_alive_period_in_seconds=600, \n",
    "    disable_output_compression=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_model_path = \"s3://sagemaker-us-west-2-365792799466/llama7b-sft-dschat-2023-10-04-05-48-33-742/output/model/\"\n",
    "critic_model_path = \"s3://sagemaker-us-west-2-365792799466/llama7b-sft-dschat-2023-10-04-06-19-18-760/output/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using provided s3_resource\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: llama7b-rlhf-dschat-2023-10-05-04-38-29-271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-10-05 04:38:30 Starting - Starting the training job...\n",
      "2023-10-05 04:38:34 Downloading - Downloading input data...........................................................................\n",
      "2023-10-05 04:53:43 Training - Training image download completed. Training in progress.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2023-10-05 04:54:16,384 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2023-10-05 04:54:16,441 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-10-05 04:54:16,451 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2023-10-05 04:54:16,453 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\n",
      "2023-10-05 04:54:16,453 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2023-10-05 04:54:17,509 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\n",
      "/opt/conda/bin/python3.9 -m pip install -r requirements.txt\n",
      "Collecting datasets>=2.8.0 (from -r requirements.txt (line 1))\n",
      "Obtaining dependency information for datasets>=2.8.0 from https://files.pythonhosted.org/packages/09/7e/fd4d6441a541dba61d0acb3c1fd5df53214c2e9033854e837a99dd9e0793/datasets-2.14.5-py3-none-any.whl.metadata\n",
      "Downloading datasets-2.14.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting sentencepiece>=0.1.97 (from -r requirements.txt (line 2))\n",
      "Downloading sentencepiece-0.1.99-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 30.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: protobuf==3.20.3 in /opt/conda/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.20.3)\n",
      "Collecting accelerate==0.23.0 (from -r requirements.txt (line 4))\n",
      "Obtaining dependency information for accelerate==0.23.0 from https://files.pythonhosted.org/packages/d9/92/2d3aecf9f4a192968035880be3e2fc8b48d541c7128f7c936f430d6f96da/accelerate-0.23.0-py3-none-any.whl.metadata\n",
      "Downloading accelerate-0.23.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting deepspeed==0.10.3 (from -r requirements.txt (line 5))\n",
      "Downloading deepspeed-0.10.3.tar.gz (867 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 867.3/867.3 kB 95.4 MB/s eta 0:00:00\n",
      "Preparing metadata (setup.py): started\n",
      "Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting transformers==4.31.0 (from -r requirements.txt (line 6))\n",
      "Obtaining dependency information for transformers==4.31.0 from https://files.pythonhosted.org/packages/21/02/ae8e595f45b6c8edee07913892b3b41f5f5f273962ad98851dc6a564bbb9/transformers-4.31.0-py3-none-any.whl.metadata\n",
      "Downloading transformers-4.31.0-py3-none-any.whl.metadata (116 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 116.9/116.9 kB 31.5 MB/s eta 0:00:00\n",
      "Collecting tensorboard (from -r requirements.txt (line 7))\n",
      "Obtaining dependency information for tensorboard from https://files.pythonhosted.org/packages/73/a2/66ed644f6ed1562e0285fcd959af17670ea313c8f331c46f79ee77187eb9/tensorboard-2.14.1-py3-none-any.whl.metadata\n",
      "Downloading tensorboard-2.14.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.23.0->-r requirements.txt (line 4)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.23.0->-r requirements.txt (line 4)) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from accelerate==0.23.0->-r requirements.txt (line 4)) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.9/site-packages (from accelerate==0.23.0->-r requirements.txt (line 4)) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.9/site-packages (from accelerate==0.23.0->-r requirements.txt (line 4)) (1.13.1+cu117)\n",
      "Collecting huggingface-hub (from accelerate==0.23.0->-r requirements.txt (line 4))\n",
      "Obtaining dependency information for huggingface-hub from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: hjson in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 5)) (3.1.0)\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 5)) (1.11.1)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 5)) (9.0.0)\n",
      "Requirement already satisfied: pydantic<2.0.0 in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 5)) (1.10.11)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from deepspeed==0.10.3->-r requirements.txt (line 5)) (4.64.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 6)) (3.12.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.31.0->-r requirements.txt (line 6))\n",
      "Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/54/71/b85c050a8b6a552261e9deae23ba20099852cfbcc9819a628ce64f5a0db6/regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 13.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from transformers==4.31.0->-r requirements.txt (line 6)) (2.31.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.31.0->-r requirements.txt (line 6))\n",
      "Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.8/7.8 MB 99.1 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.3.1 (from transformers==4.31.0->-r requirements.txt (line 6))\n",
      "Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/4c/49/d41e7f524bff04e51ebe560ecedb28127d8f4e424f9d250c106c3b7fb637/safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (12.0.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (0.3.7)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (2.0.3)\n",
      "Collecting xxhash (from datasets>=2.8.0->-r requirements.txt (line 1))\n",
      "Obtaining dependency information for xxhash from https://files.pythonhosted.org/packages/63/93/812d78f70145c68c4e64533f4d625bea01236f27698febe15f0ceebc1566/xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.8.0->-r requirements.txt (line 1)) (2023.6.0)\n",
      "Collecting aiohttp (from datasets>=2.8.0->-r requirements.txt (line 1))\n",
      "Obtaining dependency information for aiohttp from https://files.pythonhosted.org/packages/5b/8d/821fcb268cfc056964a75da3823896b17eabaa4968a2414121bc93b0c501/aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard->-r requirements.txt (line 7))\n",
      "Obtaining dependency information for absl-py>=0.4 from https://files.pythonhosted.org/packages/01/e4/dc0a1dcc4e74e08d7abedab278c795eef54a224363bb18f5692f416d834f/absl_py-2.0.0-py3-none-any.whl.metadata\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard->-r requirements.txt (line 7))\n",
      "Obtaining dependency information for grpcio>=1.48.2 from https://files.pythonhosted.org/packages/d1/a1/adf44cb808bcda1997d8afb3033b4fd503f6f5e89a6d3eeb454cb84c8abc/grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard->-r requirements.txt (line 7))\n",
      "Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/d7/88/1826b0c047c48763b36ed854a984127b430a16b70003155d7b19975f1d59/google_auth-2.23.2-py2.py3-none-any.whl.metadata\n",
      "Downloading google_auth-2.23.2-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard->-r requirements.txt (line 7))\n",
      "Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard->-r requirements.txt (line 7))\n",
      "Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (65.6.3)\n",
      "Requirement already satisfied: six>1.9 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard->-r requirements.txt (line 7))\n",
      "Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/02/52/fb9e51fba47951aabd7a6b25e41d73eae94208ccf62d886168096941a781/tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.9/site-packages (from tensorboard->-r requirements.txt (line 7)) (2.3.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1)) (3.2.0)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1))\n",
      "Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 33.2 MB/s eta 0:00:00\n",
      "Collecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1))\n",
      "Obtaining dependency information for async-timeout<5.0,>=4.0.0a3 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1))\n",
      "Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 269.4/269.4 kB 53.5 MB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1))\n",
      "Obtaining dependency information for frozenlist>=1.1.1 from https://files.pythonhosted.org/packages/b5/03/7dec2e257bd173b5ca1f74477863b97d322149f6f0284d7decead8c5ceeb/frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "Downloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.8.0->-r requirements.txt (line 1))\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7))\n",
      "Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7))\n",
      "Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 51.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (4.7.2)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 7))\n",
      "Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.23.0->-r requirements.txt (line 4)) (4.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (6.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 6)) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 6)) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->transformers==4.31.0->-r requirements.txt (line 6)) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard->-r requirements.txt (line 7)) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=2.8.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=2.8.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.9/site-packages (from pandas->datasets>=2.8.0->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->-r requirements.txt (line 7)) (3.16.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->-r requirements.txt (line 7)) (0.5.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->-r requirements.txt (line 7))\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.7/151.7 kB 41.2 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 258.1/258.1 kB 57.1 MB/s eta 0:00:00\n",
      "Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.4/7.4 MB 116.2 MB/s eta 0:00:00\n",
      "Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 519.6/519.6 kB 79.8 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 116.9 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 34.0 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.8.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 100.7 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.23.2-py2.py3-none-any.whl (181 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 182.0/182.0 kB 51.6 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.59.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.3/5.3 MB 125.8 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 295.0/295.0 kB 60.0 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.2/94.2 kB 32.6 MB/s eta 0:00:00\n",
      "Downloading regex-2023.10.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 773.3/773.3 kB 98.9 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.3.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 107.3 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 95.9 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.4.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 46.0 MB/s eta 0:00:00\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Downloading frozenlist-1.4.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (228 kB)\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 228.0/228.0 kB 57.4 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: deepspeed\n",
      "Building wheel for deepspeed (setup.py): started\n",
      "Building wheel for deepspeed (setup.py): finished with status 'done'\n",
      "Created wheel for deepspeed: filename=deepspeed-0.10.3-py3-none-any.whl size=907842 sha256=aea9719600e464ed13c12dbc0262ff94f73158a35f977d39882716a437328626\n",
      "Stored in directory: /root/.cache/pip/wheels/6c/99/c7/221a6e7ed33d72d91a5e9cdc01542dc012b69215209f2d1008\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: tokenizers, sentencepiece, safetensors, xxhash, tensorboard-data-server, regex, pyasn1-modules, oauthlib, multidict, grpcio, frozenlist, cachetools, async-timeout, absl-py, yarl, requests-oauthlib, markdown, huggingface-hub, google-auth, deepspeed, aiosignal, transformers, google-auth-oauthlib, aiohttp, accelerate, tensorboard, datasets\n",
      "Attempting uninstall: deepspeed\n",
      "Found existing installation: deepspeed 0.6.1+4c3ff1a\n",
      "Uninstalling deepspeed-0.6.1+4c3ff1a:\n",
      "Successfully uninstalled deepspeed-0.6.1+4c3ff1a\n",
      "Attempting uninstall: accelerate\n",
      "Found existing installation: accelerate 0.21.0\n",
      "Uninstalling accelerate-0.21.0:\n",
      "Successfully uninstalled accelerate-0.21.0\n",
      "Successfully installed absl-py-2.0.0 accelerate-0.23.0 aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 cachetools-5.3.1 datasets-2.14.5 deepspeed-0.10.3 frozenlist-1.4.0 google-auth-2.23.2 google-auth-oauthlib-1.0.0 grpcio-1.59.0 huggingface-hub-0.17.3 markdown-3.4.4 multidict-6.0.4 oauthlib-3.2.2 pyasn1-modules-0.3.0 regex-2023.10.3 requests-oauthlib-1.3.1 safetensors-0.3.3 sentencepiece-0.1.99 tensorboard-2.14.1 tensorboard-data-server-0.7.1 tokenizers-0.13.3 transformers-4.31.0 xxhash-3.4.1 yarl-1.9.2\n",
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "2023-10-05 04:54:40,134 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-10-05 04:54:40,134 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-10-05 04:54:40,230 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-10-05 04:54:40,306 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-10-05 04:54:40,318 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\n",
      "2023-10-05 04:54:40,380 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\n",
      "2023-10-05 04:54:40,394 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.p4de.24xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {\n",
      "        \"reward\": \"/opt/ml/input/data/reward\",\n",
      "        \"sft\": \"/opt/ml/input/data/sft\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4de.24xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"access_token\": \"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\",\n",
      "        \"actor_gradient_checkpointing\": \"\",\n",
      "        \"actor_learning_rate\": 9.65e-06,\n",
      "        \"actor_lora_dim\": 64,\n",
      "        \"actor_lora_module_name\": \"layers.\",\n",
      "        \"actor_model_name_or_path\": \"/opt/ml/input/data/sft\",\n",
      "        \"actor_weight_decay\": 0.1,\n",
      "        \"actor_zero_stage\": 3,\n",
      "        \"critic_gradient_checkpointing\": \"\",\n",
      "        \"critic_learning_rate\": 5e-06,\n",
      "        \"critic_lora_dim\": 64,\n",
      "        \"critic_lora_module_name\": \"layers.\",\n",
      "        \"critic_model_name_or_path\": \"/opt/ml/input/data/reward\",\n",
      "        \"critic_weight_decay\": 0.1,\n",
      "        \"critic_zero_stage\": 3,\n",
      "        \"data_path\": \"Dahoas/rm-static\",\n",
      "        \"data_split\": \"2,4,4\",\n",
      "        \"deepspeed\": \"\",\n",
      "        \"disable_actor_dropout\": \"\",\n",
      "        \"enable_hybrid_engine\": \"\",\n",
      "        \"enable_mixed_precision_lora\": \"\",\n",
      "        \"generation_batches\": 1,\n",
      "        \"gradient_accumulation_steps\": 1,\n",
      "        \"lr_scheduler_type\": \"cosine\",\n",
      "        \"max_answer_seq_len\": 256,\n",
      "        \"max_prompt_seq_len\": 256,\n",
      "        \"num_padding_at_beginning\": 1,\n",
      "        \"num_train_epochs\": 1,\n",
      "        \"offload_reference_model\": \"\",\n",
      "        \"output_dir\": \"/opt/ml/model\",\n",
      "        \"per_device_generation_batch_size\": 1,\n",
      "        \"per_device_training_batch_size\": 1,\n",
      "        \"ppo_epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"reward\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"sft\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4de.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"llama7b-rlhf-dschat-2023-10-05-04-38-29-271\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-365792799466/llama7b-rlhf-dschat-2023-10-05-04-38-29-271/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"rlhf/main\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4de.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4de.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"rlhf/main.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"access_token\":\"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\",\"actor_gradient_checkpointing\":\"\",\"actor_learning_rate\":9.65e-06,\"actor_lora_dim\":64,\"actor_lora_module_name\":\"layers.\",\"actor_model_name_or_path\":\"/opt/ml/input/data/sft\",\"actor_weight_decay\":0.1,\"actor_zero_stage\":3,\"critic_gradient_checkpointing\":\"\",\"critic_learning_rate\":5e-06,\"critic_lora_dim\":64,\"critic_lora_module_name\":\"layers.\",\"critic_model_name_or_path\":\"/opt/ml/input/data/reward\",\"critic_weight_decay\":0.1,\"critic_zero_stage\":3,\"data_path\":\"Dahoas/rm-static\",\"data_split\":\"2,4,4\",\"deepspeed\":\"\",\"disable_actor_dropout\":\"\",\"enable_hybrid_engine\":\"\",\"enable_mixed_precision_lora\":\"\",\"generation_batches\":1,\"gradient_accumulation_steps\":1,\"lr_scheduler_type\":\"cosine\",\"max_answer_seq_len\":256,\"max_prompt_seq_len\":256,\"num_padding_at_beginning\":1,\"num_train_epochs\":1,\"offload_reference_model\":\"\",\"output_dir\":\"/opt/ml/model\",\"per_device_generation_batch_size\":1,\"per_device_training_batch_size\":1,\"ppo_epochs\":1}\n",
      "SM_USER_ENTRY_POINT=rlhf/main.py\n",
      "SM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.p4de.24xlarge\",\"sagemaker_torch_distributed_enabled\":true}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4de.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4de.24xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"reward\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"sft\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"reward\",\"sft\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_CURRENT_INSTANCE_TYPE=ml.p4de.24xlarge\n",
      "SM_CURRENT_INSTANCE_GROUP=homogeneousCluster\n",
      "SM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\n",
      "SM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4de.24xlarge\"}}\n",
      "SM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\n",
      "SM_IS_HETERO=false\n",
      "SM_MODULE_NAME=rlhf/main\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=96\n",
      "SM_NUM_GPUS=8\n",
      "SM_NUM_NEURONS=0\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-west-2-365792799466/llama7b-rlhf-dschat-2023-10-05-04-38-29-271/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.p4de.24xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{\"reward\":\"/opt/ml/input/data/reward\",\"sft\":\"/opt/ml/input/data/sft\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4de.24xlarge\",\"distribution_hosts\":[\"algo-1\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"access_token\":\"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\",\"actor_gradient_checkpointing\":\"\",\"actor_learning_rate\":9.65e-06,\"actor_lora_dim\":64,\"actor_lora_module_name\":\"layers.\",\"actor_model_name_or_path\":\"/opt/ml/input/data/sft\",\"actor_weight_decay\":0.1,\"actor_zero_stage\":3,\"critic_gradient_checkpointing\":\"\",\"critic_learning_rate\":5e-06,\"critic_lora_dim\":64,\"critic_lora_module_name\":\"layers.\",\"critic_model_name_or_path\":\"/opt/ml/input/data/reward\",\"critic_weight_decay\":0.1,\"critic_zero_stage\":3,\"data_path\":\"Dahoas/rm-static\",\"data_split\":\"2,4,4\",\"deepspeed\":\"\",\"disable_actor_dropout\":\"\",\"enable_hybrid_engine\":\"\",\"enable_mixed_precision_lora\":\"\",\"generation_batches\":1,\"gradient_accumulation_steps\":1,\"lr_scheduler_type\":\"cosine\",\"max_answer_seq_len\":256,\"max_prompt_seq_len\":256,\"num_padding_at_beginning\":1,\"num_train_epochs\":1,\"offload_reference_model\":\"\",\"output_dir\":\"/opt/ml/model\",\"per_device_generation_batch_size\":1,\"per_device_training_batch_size\":1,\"ppo_epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"reward\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"sft\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4de.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"llama7b-rlhf-dschat-2023-10-05-04-38-29-271\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-365792799466/llama7b-rlhf-dschat-2023-10-05-04-38-29-271/source/sourcedir.tar.gz\",\"module_name\":\"rlhf/main\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4de.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4de.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"rlhf/main.py\"}\n",
      "SM_USER_ARGS=[\"--access_token\",\"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\",\"--actor_gradient_checkpointing\",\"\",\"--actor_learning_rate\",\"9.65e-06\",\"--actor_lora_dim\",\"64\",\"--actor_lora_module_name\",\"layers.\",\"--actor_model_name_or_path\",\"/opt/ml/input/data/sft\",\"--actor_weight_decay\",\"0.1\",\"--actor_zero_stage\",\"3\",\"--critic_gradient_checkpointing\",\"\",\"--critic_learning_rate\",\"5e-06\",\"--critic_lora_dim\",\"64\",\"--critic_lora_module_name\",\"layers.\",\"--critic_model_name_or_path\",\"/opt/ml/input/data/reward\",\"--critic_weight_decay\",\"0.1\",\"--critic_zero_stage\",\"3\",\"--data_path\",\"Dahoas/rm-static\",\"--data_split\",\"2,4,4\",\"--deepspeed\",\"\",\"--disable_actor_dropout\",\"\",\"--enable_hybrid_engine\",\"\",\"--enable_mixed_precision_lora\",\"\",\"--generation_batches\",\"1\",\"--gradient_accumulation_steps\",\"1\",\"--lr_scheduler_type\",\"cosine\",\"--max_answer_seq_len\",\"256\",\"--max_prompt_seq_len\",\"256\",\"--num_padding_at_beginning\",\"1\",\"--num_train_epochs\",\"1\",\"--offload_reference_model\",\"\",\"--output_dir\",\"/opt/ml/model\",\"--per_device_generation_batch_size\",\"1\",\"--per_device_training_batch_size\",\"1\",\"--ppo_epochs\",\"1\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_REWARD=/opt/ml/input/data/reward\n",
      "SM_CHANNEL_SFT=/opt/ml/input/data/sft\n",
      "SM_HP_ACCESS_TOKEN=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\n",
      "SM_HP_ACTOR_GRADIENT_CHECKPOINTING=\n",
      "SM_HP_ACTOR_LEARNING_RATE=9.65e-06\n",
      "SM_HP_ACTOR_LORA_DIM=64\n",
      "SM_HP_ACTOR_LORA_MODULE_NAME=layers.\n",
      "SM_HP_ACTOR_MODEL_NAME_OR_PATH=/opt/ml/input/data/sft\n",
      "SM_HP_ACTOR_WEIGHT_DECAY=0.1\n",
      "SM_HP_ACTOR_ZERO_STAGE=3\n",
      "SM_HP_CRITIC_GRADIENT_CHECKPOINTING=\n",
      "SM_HP_CRITIC_LEARNING_RATE=5e-06\n",
      "SM_HP_CRITIC_LORA_DIM=64\n",
      "SM_HP_CRITIC_LORA_MODULE_NAME=layers.\n",
      "SM_HP_CRITIC_MODEL_NAME_OR_PATH=/opt/ml/input/data/reward\n",
      "SM_HP_CRITIC_WEIGHT_DECAY=0.1\n",
      "SM_HP_CRITIC_ZERO_STAGE=3\n",
      "SM_HP_DATA_PATH=Dahoas/rm-static\n",
      "SM_HP_DATA_SPLIT=2,4,4\n",
      "SM_HP_DEEPSPEED=\n",
      "SM_HP_DISABLE_ACTOR_DROPOUT=\n",
      "SM_HP_ENABLE_HYBRID_ENGINE=\n",
      "SM_HP_ENABLE_MIXED_PRECISION_LORA=\n",
      "SM_HP_GENERATION_BATCHES=1\n",
      "SM_HP_GRADIENT_ACCUMULATION_STEPS=1\n",
      "SM_HP_LR_SCHEDULER_TYPE=cosine\n",
      "SM_HP_MAX_ANSWER_SEQ_LEN=256\n",
      "SM_HP_MAX_PROMPT_SEQ_LEN=256\n",
      "SM_HP_NUM_PADDING_AT_BEGINNING=1\n",
      "SM_HP_NUM_TRAIN_EPOCHS=1\n",
      "SM_HP_OFFLOAD_REFERENCE_MODEL=\n",
      "SM_HP_OUTPUT_DIR=/opt/ml/model\n",
      "SM_HP_PER_DEVICE_GENERATION_BATCH_SIZE=1\n",
      "SM_HP_PER_DEVICE_TRAINING_BATCH_SIZE=1\n",
      "SM_HP_PPO_EPOCHS=1\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python39.zip:/opt/conda/lib/python3.9:/opt/conda/lib/python3.9/lib-dynload:/opt/conda/lib/python3.9/site-packages\n",
      "Invoking script with the following command:\n",
      "torchrun --nnodes 1 --nproc_per_node 8 rlhf/main.py --access_token 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 --actor_gradient_checkpointing  --actor_learning_rate 9.65e-06 --actor_lora_dim 64 --actor_lora_module_name layers. --actor_model_name_or_path /opt/ml/input/data/sft --actor_weight_decay 0.1 --actor_zero_stage 3 --critic_gradient_checkpointing  --critic_learning_rate 5e-06 --critic_lora_dim 64 --critic_lora_module_name layers. --critic_model_name_or_path /opt/ml/input/data/reward --critic_weight_decay 0.1 --critic_zero_stage 3 --data_path Dahoas/rm-static --data_split 2,4,4 --deepspeed  --disable_actor_dropout  --enable_hybrid_engine  --enable_mixed_precision_lora  --generation_batches 1 --gradient_accumulation_steps 1 --lr_scheduler_type cosine --max_answer_seq_len 256 --max_prompt_seq_len 256 --num_padding_at_beginning 1 --num_train_epochs 1 --offload_reference_model  --output_dir /opt/ml/model --per_device_generation_batch_size 1 --per_device_training_batch_size 1 --ppo_epochs 1\n",
      "WARNING:torch.distributed.run:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2023-10-05 04:54:43,539] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:43,539] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:43,539] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:43,539] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:43,539] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:43,539] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:43,613] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:43,620] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2023-10-05 04:54:46,321] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-05 04:54:46,409] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-05 04:54:46,411] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-05 04:54:46,430] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-05 04:54:46,432] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-05 04:54:46,450] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-05 04:54:46,450] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2023-10-05 04:54:46,467] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2023-10-05 04:54:46,483] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "NCCL version 2.14.3+cuda11.7\n",
      "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 17.3MB/s]\n",
      "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 418kB/s]\n",
      "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 776/776 [00:00<00:00, 951kB/s]\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n",
      "Downloading readme:   0%|          | 0.00/530 [00:00<?, ?B/s]\n",
      "Downloading readme: 100%|██████████| 530/530 [00:00<00:00, 971kB/s]\n",
      "Downloading metadata:   0%|          | 0.00/926 [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 926/926 [00:00<00:00, 1.87MB/s]\n",
      "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0%|          | 0.00/68.4M [00:00<?, ?B/s]#033[A\n",
      "Downloading data:   6%|▌         | 4.19M/68.4M [00:00<00:03, 20.8MB/s]#033[A\n",
      "Downloading data:  18%|█▊        | 12.6M/68.4M [00:00<00:01, 41.0MB/s]#033[A\n",
      "Downloading data:  31%|███       | 21.0M/68.4M [00:00<00:00, 50.9MB/s]#033[A\n",
      "Downloading data:  43%|████▎     | 29.4M/68.4M [00:00<00:00, 56.2MB/s]#033[A\n",
      "Downloading data:  55%|█████▌    | 37.7M/68.4M [00:00<00:00, 58.8MB/s]#033[A\n",
      "Downloading data:  67%|██████▋   | 46.1M/68.4M [00:00<00:00, 61.1MB/s]#033[A\n",
      "Downloading data:  80%|███████▉  | 54.5M/68.4M [00:00<00:00, 63.1MB/s]#033[A\n",
      "Downloading data:  92%|█████████▏| 62.9M/68.4M [00:01<00:00, 64.1MB/s]#033[A\n",
      "Downloading data: 100%|██████████| 68.4M/68.4M [00:01<00:00, 56.6MB/s]\n",
      "Downloading data files:  50%|█████     | 1/2 [00:01<00:01,  1.21s/it]\n",
      "Downloading data:   0%|          | 0.00/4.61M [00:00<?, ?B/s]#033[A\n",
      "Downloading data:  91%|█████████ | 4.19M/4.61M [00:00<00:00, 32.1MB/s]#033[A\n",
      "Downloading data: 100%|██████████| 4.61M/4.61M [00:00<00:00, 35.1MB/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.73it/s]\n",
      "Downloading data files: 100%|██████████| 2/2 [00:01<00:00,  1.49it/s]\n",
      "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 2327.58it/s]\n",
      "Generating train split:   0%|          | 0/76256 [00:00<?, ? examples/s]\n",
      "Generating train split:  39%|███▉      | 30000/76256 [00:00<00:00, 200010.36 examples/s]\n",
      "Generating train split:  92%|█████████▏| 70000/76256 [00:00<00:00, 211090.38 examples/s]\n",
      "Generating train split: 100%|██████████| 76256/76256 [00:00<00:00, 210336.92 examples/s]\n",
      "Generating test split:   0%|          | 0/5103 [00:00<?, ? examples/s]\n",
      "Generating test split: 100%|██████████| 5103/5103 [00:00<00:00, 227959.07 examples/s]\n",
      "************************[start] Initializing Actor Model [start] *************************\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py39_cu117/quantizer...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/quantizer/build.ninja...\n",
      "Building extension module quantizer...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "[1/8] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=quantizer -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/quantize_int4.cu -o quantize_int4.cuda.o\n",
      "[2/8] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=quantizer -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/dequantize.cu -o dequantize.cuda.o\n",
      "[3/8] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=quantizer -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/quantize.cu -o quantize.cuda.o\n",
      "[4/8] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=quantizer -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/swizzled_quantize.cu -o swizzled_quantize.cuda.o\n",
      "[5/8] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=quantizer -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/fake_quantizer.cu -o fake_quantizer.cuda.o \n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/fake_quantizer.cu(32): warning #177-D: variable \"thread_index\" was declared but never referenced\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/fake_quantizer.cu(33): warning #177-D: variable \"reg_count\" was declared but never referenced\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/fake_quantizer.cu(107): warning #177-D: variable \"thread_index\" was declared but never referenced\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/fake_quantizer.cu(109): warning #177-D: variable \"reg_count\" was declared but never referenced\n",
      "[6/8] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=quantizer -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/pt_binding.cpp -o pt_binding.o\n",
      "[7/8] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=quantizer -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/quantization/quant_reduce.cu -o quant_reduce.cuda.o\n",
      "[8/8] c++ pt_binding.o fake_quantizer.cuda.o quantize.cuda.o quantize_int4.cuda.o dequantize.cuda.o swizzled_quantize.cuda.o quant_reduce.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o quantizer.so\n",
      "Loading extension module quantizer...\n",
      "Time to load quantizer op: 44.85431122779846 seconds\n",
      "Loading extension module quantizer...\n",
      "Time to load quantizer op: 43.877535820007324 seconds\n",
      "Loading extension module quantizer...\n",
      "Loading extension module quantizer...\n",
      "Time to load quantizer op: 43.7844877243042 seconds\n",
      "Time to load quantizer op: 43.80319309234619 seconds\n",
      "Loading extension module quantizer...\n",
      "Time to load quantizer op: 43.785757303237915 seconds\n",
      "Loading extension module quantizer...\n",
      "Time to load quantizer op: 43.881277322769165 seconds\n",
      "Loading extension module quantizer...\n",
      "Time to load quantizer op: 43.806007623672485 seconds\n",
      "Using quantizer for weights: CUDAQuantizer\n",
      "Loading extension module quantizer...\n",
      "Time to load quantizer op: 43.88319396972656 seconds\n",
      "[2023-10-05 04:56:22,567] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py39_cu117/fused_adam...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/3] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -lineinfo --use_fast_math -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -std=c++14 -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/multi_tensor_adam.cu -o multi_tensor_adam.cuda.o\n",
      "[2/3] c++ -MMD -MF fused_adam_frontend.o.d -DTORCH_EXTENSION_NAME=fused_adam -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/adam/fused_adam_frontend.cpp -o fused_adam_frontend.o\n",
      "[3/3] c++ fused_adam_frontend.o multi_tensor_adam.cuda.o -shared -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o fused_adam.so\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.27323317527771 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.233713626861572 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.231845140457153 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.333821296691895 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.333701372146606 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.332630157470703 seconds\n",
      "[2023-10-05 04:56:52,331] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.332101345062256 seconds\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 26.33105969429016 seconds\n",
      "[2023-10-05 04:56:52,397] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-10-05 04:56:52,399] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-10-05 04:56:52,399] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-10-05 04:56:52,441] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2023-10-05 04:56:52,441] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2023-10-05 04:56:52,441] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2023-10-05 04:56:52,441] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2023-10-05 04:56:52,749] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-10-05 04:56:52,750] [INFO] [utils.py:804:see_memory_usage] MA 1.94 GB         Max_MA 2.41 GB         CA 3.38 GB         Max_CA 3 GB\n",
      "[2023-10-05 04:56:52,750] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:52,753] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000\n",
      "[2023-10-05 04:56:52,753] [INFO] [stage3.py:127:__init__] Prefetch bucket size 30000000\n",
      "[2023-10-05 04:56:52,998] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-10-05 04:56:52,999] [INFO] [utils.py:804:see_memory_usage] MA 1.94 GB         Max_MA 1.94 GB         CA 3.38 GB         Max_CA 3 GB\n",
      "[2023-10-05 04:56:52,999] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2023-10-05 04:56:53,367] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-10-05 04:56:53,368] [INFO] [utils.py:804:see_memory_usage] MA 1.67 GB         Max_MA 1.94 GB         CA 3.38 GB         Max_CA 3 GB\n",
      "[2023-10-05 04:56:53,368] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:53,602] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-10-05 04:56:53,603] [INFO] [utils.py:804:see_memory_usage] MA 1.67 GB         Max_MA 1.67 GB         CA 3.38 GB         Max_CA 3 GB\n",
      "[2023-10-05 04:56:53,603] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:54,349] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 2\n",
      "[2023-10-05 04:56:54,350] [INFO] [utils.py:804:see_memory_usage] MA 1.67 GB         Max_MA 1.67 GB         CA 1.98 GB         Max_CA 3 GB\n",
      "[2023-10-05 04:56:54,350] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.68 GB, percent = 4.1%\n",
      "[2023-10-05 04:56:54,586] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-10-05 04:56:54,586] [INFO] [utils.py:804:see_memory_usage] MA 1.67 GB         Max_MA 1.67 GB         CA 1.98 GB         Max_CA 2 GB\n",
      "[2023-10-05 04:56:54,586] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:54,820] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\n",
      "[2023-10-05 04:56:54,821] [INFO] [utils.py:804:see_memory_usage] MA 1.87 GB         Max_MA 1.91 GB         CA 2.18 GB         Max_CA 2 GB\n",
      "[2023-10-05 04:56:54,821] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:55,057] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\n",
      "[2023-10-05 04:56:55,058] [INFO] [utils.py:804:see_memory_usage] MA 1.87 GB         Max_MA 1.87 GB         CA 2.18 GB         Max_CA 2 GB\n",
      "[2023-10-05 04:56:55,058] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:55,293] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\n",
      "[2023-10-05 04:56:55,294] [INFO] [utils.py:804:see_memory_usage] MA 2.27 GB         Max_MA 2.39 GB         CA 2.7 GB         Max_CA 3 GB\n",
      "[2023-10-05 04:56:55,294] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:55,294] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2023-10-05 04:56:55,796] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-10-05 04:56:55,797] [INFO] [utils.py:804:see_memory_usage] MA 3.3 GB         Max_MA 3.78 GB         CA 4.38 GB         Max_CA 4 GB\n",
      "[2023-10-05 04:56:55,797] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 45.29 GB, percent = 4.0%\n",
      "[2023-10-05 04:56:55,797] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam\n",
      "[2023-10-05 04:56:55,798] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-10-05 04:56:55,798] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f51f78282e0>\n",
      "[2023-10-05 04:56:55,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2023-10-05 04:56:55,799] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\n",
      "[2023-10-05 04:56:55,799] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-10-05 04:56:55,799] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-10-05 04:56:55,799] [INFO] [config.py:971:print]   amp_enabled .................. False\n",
      "[2023-10-05 04:56:55,799] [INFO] [config.py:971:print]   amp_params ................... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5202fc5460>\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   communication_data_type ...... None\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   disable_allgather ............ False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   dump_state ................... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   elasticity_enabled ........... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   fp16_enabled ................. True\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   global_rank .................. 0\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=True max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   loss_scale ................... 0\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   memory_breakdown ............. False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   mics_shard_size .............. -1\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step3_tensorboard/ds_tensorboard_logs/', job_name='step3_actor_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-10-05 04:56:55,800] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   optimizer_name ............... None\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   optimizer_params ............. None\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   pld_enabled .................. False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   pld_params ................... False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   prescale_gradients ........... False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   scheduler_name ............... None\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   scheduler_params ............. None\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   sparse_attention ............. None\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   steps_per_print .............. 10\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   train_batch_size ............. 8\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   use_node_local_storage ....... False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   weight_quantization_config ... None\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   world_size ................... 8\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=True zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   zero_enabled ................. True\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\n",
      "[2023-10-05 04:56:55,801] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false, \n",
      "        \"zero_quantized_nontrainable_weights\": true\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale_window\": 1000\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": true, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": false, \n",
      "        \"output_path\": \"step3_tensorboard/ds_tensorboard_logs/\", \n",
      "        \"job_name\": \"step3_actor_tensorboard\"\n",
      "    }\n",
      "}\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /root/.cache/torch_extensions/py39_cu117/transformer_inference...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py39_cu117/transformer_inference/build.ninja...\n",
      "Building extension module transformer_inference...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pointwise_ops.cu -o pointwise_ops.cuda.o\n",
      "[2/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/relu.cu -o relu.cuda.o\n",
      "[3/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/dequantize.cu -o dequantize.cuda.o\n",
      "[4/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu -o transform.cuda.o \n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(63): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(103): warning #177-D: variable \"half_dim\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(263): here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(120): warning #177-D: variable \"vals_half\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(263): here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(121): warning #177-D: variable \"output_half\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(263): here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/transform.cu(138): warning #177-D: variable \"lane\" was declared but never referenced\n",
      "          detected during instantiation of \"void launch_bias_add_transform_0213(T *, T *, T *, const T *, const T *, int, int, unsigned int, int, int, int, int, __nv_bool, __nv_bool, cudaStream_t, int, int) [with T=__nv_bfloat16]\" \n",
      "(263): here\n",
      "[5/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/gelu.cu -o gelu.cuda.o\n",
      "[6/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/apply_rotary_pos_emb.cu -o apply_rotary_pos_emb.cuda.o\n",
      "[7/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/rms_norm.cu -o rms_norm.cuda.o\n",
      "[8/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/softmax.cu -o softmax.cuda.o\n",
      "[9/11] /usr/local/cuda/bin/nvcc  -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_80,code=compute_80 -gencode=arch=compute_80,code=sm_80 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -U__CUDA_NO_HALF_OPERATORS__ -U__CUDA_NO_HALF_CONVERSIONS__ -U__CUDA_NO_HALF2_OPERATORS__ -gencode=arch=compute_80,code=sm_80 -gencode=arch=compute_80,code=compute_80 -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/layer_norm.cu -o layer_norm.cuda.o\n",
      "[10/11] c++ -MMD -MF pt_binding.o.d -DTORCH_EXTENSION_NAME=transformer_inference -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/includes -I/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/includes -isystem /opt/conda/lib/python3.9/site-packages/torch/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/torch/csrc/api/include -isystem /opt/conda/lib/python3.9/site-packages/torch/include/TH -isystem /opt/conda/lib/python3.9/site-packages/torch/include/THC -isystem /usr/local/cuda/include -isystem /opt/conda/include/python3.9 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++14 -O3 -std=c++17 -g -Wno-reorder -DBF16_AVAILABLE -c /opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp -o pt_binding.o\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = float]’:\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1978:5:   required from here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  536 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  537 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  545 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  546 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = float]’:\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1978:5:   required from here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1571:72: warning: narrowing conversion of ‘(size_t)mlp_1_out_neurons’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      " 1571 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1571:72: warning: narrowing conversion of ‘mlp_1_out_neurons’ from ‘const size_t’ {aka ‘const long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = __half]’:\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1979:5:   required from here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  536 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  537 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  545 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  546 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = __half]’:\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1979:5:   required from here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1571:72: warning: narrowing conversion of ‘(size_t)mlp_1_out_neurons’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      " 1571 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1571:72: warning: narrowing conversion of ‘mlp_1_out_neurons’ from ‘const size_t’ {aka ‘const long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_softmax_context(at::Tensor&, at::Tensor&, int, bool, bool, int, float, bool, bool, int, bool, unsigned int, unsigned int, at::Tensor&) [with T = __nv_bfloat16]’:\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1981:5:   required from here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  536 |                                      {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:536:50: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  537 |                                       k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                                       ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:537:41: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  545 |                          {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:545:38: warning: narrowing conversion of ‘(((size_t)hidden_dim) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "  546 |                           k * InferenceContext::Instance().GetMaxTokenLength(),\n",
      "      |                           ~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:546:29: warning: narrowing conversion of ‘(((size_t)k) * (& InferenceContext::Instance())->InferenceContext::GetMaxTokenLength())’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp: In instantiation of ‘std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, at::Tensor&, float, at::Tensor&, at::Tensor&, bool, int, bool) [with T = __nv_bfloat16]’:\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1981:5:   required from here\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1571:72: warning: narrowing conversion of ‘(size_t)mlp_1_out_neurons’ from ‘size_t’ {aka ‘long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      " 1571 |         at::from_blob(intermediate_ptr, {input.size(0), input.size(1), mlp_1_out_neurons}, options);\n",
      "      |                                                                        ^~~~~~~~~~~~~~~~~\n",
      "/opt/conda/lib/python3.9/site-packages/deepspeed/ops/csrc/transformer/inference/csrc/pt_binding.cpp:1571:72: warning: narrowing conversion of ‘mlp_1_out_neurons’ from ‘const size_t’ {aka ‘const long unsigned int’} to ‘long int’ [-Wnarrowing]\n",
      "[11/11] c++ pt_binding.o gelu.cuda.o relu.cuda.o layer_norm.cuda.o rms_norm.cuda.o softmax.cuda.o dequantize.cuda.o apply_rotary_pos_emb.cuda.o transform.cuda.o pointwise_ops.cuda.o -shared -lcurand -L/opt/conda/lib/python3.9/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda_cu -ltorch_cuda_cpp -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o transformer_inference.so\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 32.739508628845215 seconds\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 32.784245014190674 seconds\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 32.783780574798584 seconds\n",
      "Loading extension module transformer_inference...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 32.78468155860901 seconds\n",
      "Time to load transformer_inference op: 32.78401589393616 seconds\n",
      "Loading extension module transformer_inference...Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 32.79051947593689 seconds\n",
      "Time to load transformer_inference op: 32.790719985961914 seconds\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 32.79279017448425 seconds\n",
      "[2023-10-05 04:57:28,715] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed-Inference config: {'layer_id': 0, 'hidden_size': 4096, 'intermediate_size': 11008, 'heads': 32, 'num_hidden_layers': -1, 'dtype': torch.float16, 'pre_layer_norm': True, 'norm_type': <NormType.RMSNorm: 3>, 'local_rank': -1, 'stochastic_mode': False, 'epsilon': 1e-05, 'mp_size': 1, 'scale_attention': True, 'triangular_masking': True, 'local_attention': False, 'window_size': 1, 'rotary_dim': 128, 'rotate_half': True, 'rotate_every_two': False, 'return_tuple': True, 'mlp_after_attn': True, 'mlp_act_func_type': <ActivationFuncType.GATED_SILU: 4>, 'specialized_mode': False, 'training_mp_size': 1, 'bigscience_bloom': False, 'max_out_tokens': 512, 'min_out_tokens': 512, 'scale_attn_by_inverse_layer_idx': False, 'enable_qkv_quantization': False, 'use_mup': False, 'return_single_tuple': False, 'set_empty_params': True, 'transposed_mode': True, 'use_triton': False, 'triton_autotune': False}\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.03898763656616211 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.043726205825805664 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.046442508697509766 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.045823097229003906 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04630875587463379 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04660916328430176 seconds\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.046445369720458984 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04855966567993164 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.039444923400878906 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04347491264343262 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04478621482849121 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04566168785095215 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.046724796295166016 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.04798412322998047 seconds\n",
      "Time to load transformer_inference op: 0.04773998260498047 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module transformer_inference, skipping build step...\n",
      "Loading extension module transformer_inference...\n",
      "Time to load transformer_inference op: 0.0490880012512207 seconds\n",
      "*****************[end] Initialized Actor Model [end] (duration: 120.05s)******************\n",
      "*************************[start] Initializing Ref Model [start] **************************\n",
      "[2023-10-05 04:57:37,964] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 582, num_elems = 13.48B\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /opt/ml/input/data/sft and are newly initialized: ['model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[2023-10-05 04:57:41,488] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\n",
      "[2023-10-05 04:57:41,529] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-10-05 04:57:41,531] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2023-10-05 04:57:41,813] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-10-05 04:57:41,814] [INFO] [utils.py:804:see_memory_usage] MA 3.62 GB         Max_MA 4.43 GB         CA 4.92 GB         Max_CA 5 GB\n",
      "[2023-10-05 04:57:41,814] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 56.98 GB, percent = 5.1%\n",
      "Parameter Offload: Total persistent parameters: 266240 in 65 params\n",
      "[2023-10-05 04:57:42,074] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-10-05 04:57:42,074] [INFO] [utils.py:804:see_memory_usage] MA 3.62 GB         Max_MA 3.62 GB         CA 4.92 GB         Max_CA 5 GB\n",
      "[2023-10-05 04:57:42,075] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 56.98 GB, percent = 5.1%\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   amp_enabled .................. False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   amp_params ................... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5203d773d0>\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   communication_data_type ...... None\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   disable_allgather ............ False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   dump_state ................... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\n",
      "[2023-10-05 04:57:42,076] [INFO] [config.py:971:print]   elasticity_enabled ........... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   fp16_enabled ................. True\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   global_rank .................. 0\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   loss_scale ................... 0\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   memory_breakdown ............. False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   mics_shard_size .............. -1\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   optimizer_name ............... None\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   optimizer_params ............. None\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   pld_enabled .................. False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   pld_params ................... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   prescale_gradients ........... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   scheduler_name ............... None\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   scheduler_params ............. None\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   sparse_attention ............. None\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   steps_per_print .............. 10\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   train_batch_size ............. 8\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   use_node_local_storage ....... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   weight_quantization_config ... None\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   world_size ................... 8\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   zero_enabled ................. True\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\n",
      "[2023-10-05 04:57:42,077] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false\n",
      "}\n",
      "*******************[end] Initialized Ref Model [end] (duration: 13.03s)*******************\n",
      "************************[start] Initializing Critic Model [start] ************************\n",
      "[2023-10-05 04:57:42,543] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 872, num_elems = 20.08B\n",
      "> Creating model from_config took 0.4706146717071533 seconds\n",
      "> torch.load took 7.040448427200317 seconds\n",
      "> Loading model state dict took 3.381732702255249 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0010578632354736328 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0010039806365966797 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0010440349578857422 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0009181499481201172 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0009052753448486328 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0008842945098876953 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0008895397186279297 seconds\n",
      "Using /root/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "No modifications detected for re-loaded extension module fused_adam, skipping build step...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.0009982585906982422 seconds\n",
      "[2023-10-05 04:57:54,160] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\n",
      "[2023-10-05 04:57:54,191] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-10-05 04:57:54,192] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the client Optimizer\n",
      "[2023-10-05 04:57:54,192] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2023-10-05 04:57:54,231] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2023-10-05 04:57:54,231] [INFO] [utils.py:54:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2023-10-05 04:57:54,231] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2023-10-05 04:57:54,231] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2023-10-05 04:57:54,524] [INFO] [utils.py:803:see_memory_usage] Stage 3 initialize beginning\n",
      "[2023-10-05 04:57:54,524] [INFO] [utils.py:804:see_memory_usage] MA 5.53 GB         Max_MA 5.8 GB         CA 14.7 GB         Max_CA 15 GB\n",
      "[2023-10-05 04:57:54,525] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 58.0 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:54,527] [INFO] [stage3.py:126:__init__] Reduce bucket size 500,000,000\n",
      "[2023-10-05 04:57:54,527] [INFO] [stage3.py:127:__init__] Prefetch bucket size 30000000\n",
      "[2023-10-05 04:57:54,780] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-10-05 04:57:54,781] [INFO] [utils.py:804:see_memory_usage] MA 5.53 GB         Max_MA 5.53 GB         CA 14.7 GB         Max_CA 15 GB\n",
      "[2023-10-05 04:57:54,781] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 58.0 GB, percent = 5.2%\n",
      "Parameter Offload: Total persistent parameters: 270336 in 66 params\n",
      "[2023-10-05 04:57:55,171] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-10-05 04:57:55,172] [INFO] [utils.py:804:see_memory_usage] MA 5.26 GB         Max_MA 5.53 GB         CA 14.7 GB         Max_CA 15 GB\n",
      "[2023-10-05 04:57:55,172] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 58.0 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:55,433] [INFO] [utils.py:803:see_memory_usage] Before creating fp16 partitions\n",
      "[2023-10-05 04:57:55,433] [INFO] [utils.py:804:see_memory_usage] MA 5.26 GB         Max_MA 5.26 GB         CA 14.7 GB         Max_CA 15 GB\n",
      "[2023-10-05 04:57:55,434] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 58.0 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:56,337] [INFO] [utils.py:803:see_memory_usage] After creating fp16 partitions: 2\n",
      "[2023-10-05 04:57:56,339] [INFO] [utils.py:804:see_memory_usage] MA 5.26 GB         Max_MA 5.26 GB         CA 5.74 GB         Max_CA 15 GB\n",
      "[2023-10-05 04:57:56,339] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 58.0 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:56,600] [INFO] [utils.py:803:see_memory_usage] Before creating fp32 partitions\n",
      "[2023-10-05 04:57:56,600] [INFO] [utils.py:804:see_memory_usage] MA 5.26 GB         Max_MA 5.26 GB         CA 5.74 GB         Max_CA 6 GB\n",
      "[2023-10-05 04:57:56,600] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 57.99 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:56,861] [INFO] [utils.py:803:see_memory_usage] After creating fp32 partitions\n",
      "[2023-10-05 04:57:56,861] [INFO] [utils.py:804:see_memory_usage] MA 5.4 GB         Max_MA 5.43 GB         CA 5.74 GB         Max_CA 6 GB\n",
      "[2023-10-05 04:57:56,861] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 57.99 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:57,122] [INFO] [utils.py:803:see_memory_usage] Before initializing optimizer states\n",
      "[2023-10-05 04:57:57,123] [INFO] [utils.py:804:see_memory_usage] MA 5.4 GB         Max_MA 5.4 GB         CA 5.74 GB         Max_CA 6 GB\n",
      "[2023-10-05 04:57:57,123] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 57.99 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:57,386] [INFO] [utils.py:803:see_memory_usage] After initializing optimizer states\n",
      "[2023-10-05 04:57:57,386] [INFO] [utils.py:804:see_memory_usage] MA 5.67 GB         Max_MA 5.74 GB         CA 6.03 GB         Max_CA 6 GB\n",
      "[2023-10-05 04:57:57,386] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 57.99 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:57,387] [INFO] [stage3.py:448:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2023-10-05 04:57:57,912] [INFO] [utils.py:803:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2023-10-05 04:57:57,912] [INFO] [utils.py:804:see_memory_usage] MA 6.67 GB         Max_MA 7.16 GB         CA 7.71 GB         Max_CA 8 GB\n",
      "[2023-10-05 04:57:57,913] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 57.99 GB, percent = 5.2%\n",
      "[2023-10-05 04:57:57,913] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam\n",
      "[2023-10-05 04:57:57,913] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2023-10-05 04:57:57,913] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0x7f5218921ca0>\n",
      "[2023-10-05 04:57:57,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2023-10-05 04:57:57,914] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\n",
      "[2023-10-05 04:57:57,914] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-10-05 04:57:57,914] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-10-05 04:57:57,914] [INFO] [config.py:971:print]   amp_enabled .................. False\n",
      "[2023-10-05 04:57:57,914] [INFO] [config.py:971:print]   amp_params ................... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5218135b20>\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   communication_data_type ...... None\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   disable_allgather ............ False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   dump_state ................... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   elasticity_enabled ........... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   fp16_enabled ................. True\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   global_rank .................. 0\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   loss_scale ................... 0\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   memory_breakdown ............. False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   mics_shard_size .............. -1\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='step3_tensorboard/ds_tensorboard_logs/', job_name='step3_critic_tensorboard') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   optimizer_name ............... None\n",
      "[2023-10-05 04:57:57,915] [INFO] [config.py:971:print]   optimizer_params ............. None\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   pld_enabled .................. False\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   pld_params ................... False\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   prescale_gradients ........... False\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   scheduler_name ............... None\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   scheduler_params ............. None\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   sparse_attention ............. None\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   steps_per_print .............. 10\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   train_batch_size ............. 8\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   use_node_local_storage ....... False\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   weight_quantization_config ... None\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   world_size ................... 8\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=30000000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   zero_enabled ................. True\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\n",
      "[2023-10-05 04:57:57,916] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale_window\": 1000\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": false, \n",
      "        \"output_path\": \"step3_tensorboard/ds_tensorboard_logs/\", \n",
      "        \"job_name\": \"step3_critic_tensorboard\"\n",
      "    }\n",
      "}\n",
      "*****************[end] Initialized Critic Model [end] (duration: 15.84s)******************\n",
      "************************[start] Initializing Reward Model [start] ************************\n",
      "[2023-10-05 04:57:58,338] [INFO] [partition_parameters.py:347:__exit__] finished initializing model - num_params = 1162, num_elems = 26.69B\n",
      "> Creating model from_config took 0.42850780487060547 seconds\n",
      "> torch.load took 7.187354803085327 seconds\n",
      "> Loading model state dict took 1.8309576511383057 seconds\n",
      "[2023-10-05 04:58:07,777] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.10.3, git-hash=unknown, git-branch=unknown\n",
      "[2023-10-05 04:58:07,797] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2023-10-05 04:58:07,799] [INFO] [logging.py:96:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2023-10-05 04:58:08,077] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2023-10-05 04:58:08,077] [INFO] [utils.py:804:see_memory_usage] MA 8.27 GB         Max_MA 8.84 GB         CA 17.92 GB         Max_CA 18 GB\n",
      "[2023-10-05 04:58:08,078] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 58.03 GB, percent = 5.2%\n",
      "Mixed Precision ZeRO++ enabled\n",
      "Mixed Precision ZeRO++ enabledMixed Precision ZeRO++ enabled\n",
      "Mixed Precision ZeRO++ enabled\n",
      "Mixed Precision ZeRO++ enabled\n",
      "Mixed Precision ZeRO++ enabledMixed Precision ZeRO++ enabled\n",
      "Parameter Offload: Total persistent parameters: 270336 in 66 params\n",
      "[2023-10-05 04:58:08.101 algo-1:391 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.101 algo-1:395 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.101 algo-1:394 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.101 algo-1:393 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.102 algo-1:397 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.102 algo-1:396 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.102 algo-1:392 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.150 algo-1:393 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.150 algo-1:394 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.150 algo-1:391 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.150 algo-1:395 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.151 algo-1:394 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.151 algo-1:393 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.151 algo-1:391 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.151 algo-1:395 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.151 algo-1:393 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.151 algo-1:394 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.151 algo-1:391 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.151 algo-1:395 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.151 algo-1:397 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.152 algo-1:394 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.152 algo-1:393 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.152 algo-1:392 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.152 algo-1:393 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-10-05 04:58:08.152 algo-1:394 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-10-05 04:58:08.152 algo-1:391 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.152 algo-1:395 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.152 algo-1:391 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-10-05 04:58:08.152 algo-1:395 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-10-05 04:58:08.152 algo-1:396 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.152 algo-1:397 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.152 algo-1:392 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.152 algo-1:397 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.153 algo-1:396 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.153 algo-1:392 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.153 algo-1:396 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.153 algo-1:397 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.153 algo-1:397 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-10-05 04:58:08.153 algo-1:392 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.153 algo-1:392 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-10-05 04:58:08.153 algo-1:396 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.153 algo-1:396 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2023-10-05 04:58:08,420] [INFO] [utils.py:803:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2023-10-05 04:58:08,421] [INFO] [utils.py:804:see_memory_usage] MA 8.27 GB         Max_MA 8.27 GB         CA 17.92 GB         Max_CA 18 GB\n",
      "[2023-10-05 04:58:08,421] [INFO] [utils.py:811:see_memory_usage] CPU Virtual Memory:  used = 58.04 GB, percent = 5.2%\n",
      "[2023-10-05 04:58:08,422] [INFO] [config.py:967:print] DeepSpeedEngine configuration:\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   amp_enabled .................. False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   amp_params ................... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   bfloat16_enabled ............. False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5230cb8a60>\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   communication_data_type ...... None\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   curriculum_enabled_legacy .... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   curriculum_params_legacy ..... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   data_efficiency_enabled ...... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   dataloader_drop_last ......... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   disable_allgather ............ False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   dump_state ................... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   dynamic_loss_scale_args ...... None\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_enabled ........... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   eigenvalue_verbose ........... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   elasticity_enabled ........... False\n",
      "[2023-10-05 04:58:08,423] [INFO] [config.py:971:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   fp16_auto_cast ............... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   fp16_enabled ................. True\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   global_rank .................. 0\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   grad_accum_dtype ............. None\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   gradient_accumulation_steps .. 1\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   gradient_clipping ............ 1.0\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   initial_dynamic_scale ........ 65536\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   load_universal_checkpoint .... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   loss_scale ................... 0\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   memory_breakdown ............. False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   mics_hierarchial_params_gather  False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   mics_shard_size .............. -1\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   optimizer_name ............... None\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   optimizer_params ............. None\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   pld_enabled .................. False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   pld_params ................... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   prescale_gradients ........... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   scheduler_name ............... None\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   scheduler_params ............. None\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   sparse_attention ............. None\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   sparse_gradients_enabled ..... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   steps_per_print .............. 10\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   train_batch_size ............. 8\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   train_micro_batch_size_per_gpu  1\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   use_node_local_storage ....... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   wall_clock_breakdown ......... False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   weight_quantization_config ... None\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   world_size ................... 8\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   zero_allow_untested_optimizer  False\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=10000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   zero_enabled ................. True\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:971:print]   zero_optimization_stage ...... 3\n",
      "[2023-10-05 04:58:08,424] [INFO] [config.py:957:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": 10, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false\n",
      "}\n",
      "*****************[end] Initialized Reward Model [end] (duration: 10.51s)******************\n",
      "Mixed Precision ZeRO++ enabled\n",
      "***** Running training *****\n",
      "Beginning of Epoch 1/1, Total Generation Batches 3813\n",
      "[2023-10-05 04:58:08.453 algo-1:390 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-10-05 04:58:08.503 algo-1:390 INFO profiler_config_parser.py:111] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "[2023-10-05 04:58:08.504 algo-1:390 INFO json_config.py:92] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2023-10-05 04:58:08.504 algo-1:390 INFO hook.py:206] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2023-10-05 04:58:08.505 algo-1:390 INFO hook.py:259] Saving to /opt/ml/output/tensors\n",
      "[2023-10-05 04:58:08.505 algo-1:390 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "------------------------------------------------------\n",
      "Free memory : 51.187317 (GigaBytes)  \n",
      "Total memory: 79.325012 (GigaBytes)  \n",
      "Requested memory: 0.609375 (GigaBytes) \n",
      "Setting maximum total tokens (input + output) to 512 \n",
      "WorkSpace: 0x7f4852000000 \n",
      "------------------------------------------------------\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "/opt/conda/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n",
      "[2023-10-05 04:58:22,931] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "[2023-10-05 04:58:24,465] [INFO] [loss_scaler.py:190:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, but hysteresis is 2. Reducing hysteresis to 1\n",
      "Epoch: 0 | Step: 0 | PPO Epoch: 1 | Actor Loss: -0.09104315936565399 | Critic Loss: 0.08153443783521652 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 13.79s, TFLOPs: 3.50, Samples/sec: 0.58, Time/seq 1.72s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 8.95s, Per-token Latency 34.96 ms, TFLOPs: 0.76, BW: 394.68 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 4.84s, TFLOPs: 8.56\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.73828125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=15.62s |Gather latency=0.70s (4.50%) |Generate time=8.24s (52.77%) |Training time=5.12s (32.80%) |Others=2.25 (14.43%)|CurSamplesPerSec=0.51 |AvgSamplesPerSec=0.51\n",
      "[2023-10-05 04:58:38,331] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2023-10-05 04:58:40,467] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "Epoch: 0 | Step: 1 | PPO Epoch: 1 | Actor Loss: 0.04133332148194313 | Critic Loss: 0.08230999857187271 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 12.30s, TFLOPs: 3.92, Samples/sec: 0.65, Time/seq 1.54s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.94s, Per-token Latency 31.00 ms, TFLOPs: 0.86, BW: 445.10 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 4.37s, TFLOPs: 9.49\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.0078125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=16.00s |Gather latency=0.75s (4.70%) |Generate time=7.18s (44.88%) |Training time=4.95s (30.93%) |Others=3.87 (24.19%)|CurSamplesPerSec=0.50 |AvgSamplesPerSec=0.51\n",
      "[2023-10-05 04:58:52,944] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2023-10-05 04:58:54,195] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "Epoch: 0 | Step: 2 | PPO Epoch: 1 | Actor Loss: -0.03173011541366577 | Critic Loss: 0.1428094208240509 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.67s, TFLOPs: 4.53, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 8.06s, Per-token Latency 31.47 ms, TFLOPs: 0.85, BW: 438.46 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.61s, TFLOPs: 15.88\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.62890625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.73s |Gather latency=0.76s (5.51%) |Generate time=7.30s (53.16%) |Training time=3.28s (23.93%) |Others=3.15 (22.91%)|CurSamplesPerSec=0.58 |AvgSamplesPerSec=0.53\n",
      "[2023-10-05 04:59:06,676] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "[2023-10-05 04:59:07,927] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "Epoch: 0 | Step: 3 | PPO Epoch: 1 | Actor Loss: -0.2056131213903427 | Critic Loss: 0.1053462028503418 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.65s, TFLOPs: 4.53, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 8.04s, Per-token Latency 31.41 ms, TFLOPs: 0.85, BW: 439.21 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.61s, TFLOPs: 15.88\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.544921875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.73s |Gather latency=0.77s (5.59%) |Generate time=7.27s (52.95%) |Training time=3.29s (23.95%) |Others=3.17 (23.10%)|CurSamplesPerSec=0.58 |AvgSamplesPerSec=0.54\n",
      "[2023-10-05 04:59:20,401] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "[2023-10-05 04:59:21,651] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "Epoch: 0 | Step: 4 | PPO Epoch: 1 | Actor Loss: -0.037841446697711945 | Critic Loss: 0.10821831971406937 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.62s, TFLOPs: 4.55, Samples/sec: 0.75, Time/seq 1.33s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 8.01s, Per-token Latency 31.27 ms, TFLOPs: 0.85, BW: 441.16 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.61s, TFLOPs: 15.88\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.85546875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.72s |Gather latency=0.73s (5.30%) |Generate time=7.28s (53.02%) |Training time=3.30s (24.06%) |Others=3.15 (22.92%)|CurSamplesPerSec=0.58 |AvgSamplesPerSec=0.55\n",
      "[2023-10-05 04:59:33,103] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "[2023-10-05 04:59:34,291] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "Epoch: 0 | Step: 5 | PPO Epoch: 1 | Actor Loss: -0.0430045947432518 | Critic Loss: 0.14721983671188354 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.72s, TFLOPs: 4.96, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.25s, Per-token Latency 28.31 ms, TFLOPs: 0.94, BW: 487.28 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.48s, TFLOPs: 16.74\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.853515625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.64s |Gather latency=0.77s (6.09%) |Generate time=6.48s (51.25%) |Training time=3.04s (24.04%) |Others=3.12 (24.71%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.56\n",
      "Epoch: 0 | Step: 6 | PPO Epoch: 1 | Actor Loss: -0.10510008782148361 | Critic Loss: 0.13727036118507385 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.79s, TFLOPs: 4.93, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.26s, Per-token Latency 28.36 ms, TFLOPs: 0.94, BW: 486.45 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.53s, TFLOPs: 16.36\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.85546875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.68s |Gather latency=0.78s (6.13%) |Generate time=6.48s (51.11%) |Training time=3.07s (24.17%) |Others=3.13 (24.72%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.57\n",
      "Epoch: 0 | Step: 7 | PPO Epoch: 1 | Actor Loss: -0.028118908405303955 | Critic Loss: 0.09829417616128922 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.76s, TFLOPs: 4.95, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.24s, Per-token Latency 28.29 ms, TFLOPs: 0.94, BW: 487.75 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.51s, TFLOPs: 16.48\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.69921875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.60s |Gather latency=0.72s (5.75%) |Generate time=6.52s (51.71%) |Training time=3.03s (24.03%) |Others=3.06 (24.26%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.58\n",
      "[2023-10-05 05:00:12,130] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "Epoch: 0 | Step: 8 | PPO Epoch: 1 | Actor Loss: -0.07823660224676132 | Critic Loss: 0.1105414554476738 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.74s, TFLOPs: 4.95, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.26s, Per-token Latency 28.35 ms, TFLOPs: 0.94, BW: 486.73 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.49s, TFLOPs: 16.66\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.8125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.56s |Gather latency=0.77s (6.16%) |Generate time=6.48s (51.62%) |Training time=3.02s (24.01%) |Others=3.06 (24.36%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.58\n",
      "[2023-10-05 05:00:23,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=6, lr=[3.8600000000000004e-07, 2e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2023-10-05 05:00:23,542] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=6.135322304833954, CurrSamplesPerSec=6.208769547408102, MemAllocated=8.17GB, MaxMemAllocated=26.53GB\n",
      "[2023-10-05 05:00:24,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=7, lr=[1.5000000000000002e-07, 1.5e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "Epoch: 0 | Step: 9 | PPO Epoch: 1 | Actor Loss: -0.04938121885061264 | Critic Loss: 0.08047368377447128 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.79s, TFLOPs: 4.93, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.26s, Per-token Latency 28.34 ms, TFLOPs: 0.94, BW: 486.81 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.53s, TFLOPs: 16.37\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.95703125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.63s |Gather latency=0.73s (5.75%) |Generate time=6.53s (51.66%) |Training time=3.04s (24.06%) |Others=3.07 (24.28%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.59\n",
      "[2023-10-05 05:00:36,123] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "Epoch: 0 | Step: 10 | PPO Epoch: 1 | Actor Loss: -0.024884190410375595 | Critic Loss: 0.1410224437713623 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.80s, TFLOPs: 4.93, Samples/sec: 0.82, Time/seq 1.23s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.31s, Per-token Latency 28.57 ms, TFLOPs: 0.93, BW: 482.93 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.49s, TFLOPs: 16.66\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.2578125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.57s |Gather latency=0.81s (6.43%) |Generate time=6.50s (51.75%) |Training time=2.99s (23.81%) |Others=3.07 (24.44%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.59\n",
      "Epoch: 0 | Step: 11 | PPO Epoch: 1 | Actor Loss: -0.04269332066178322 | Critic Loss: 0.09243184328079224 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.73s, TFLOPs: 4.96, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.21s, Per-token Latency 28.15 ms, TFLOPs: 0.95, BW: 490.09 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.53s, TFLOPs: 16.41\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.865234375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.66s |Gather latency=0.72s (5.65%) |Generate time=6.49s (51.25%) |Training time=3.08s (24.35%) |Others=3.09 (24.40%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.60\n",
      "[2023-10-05 05:01:02,544] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
      "Epoch: 0 | Step: 12 | PPO Epoch: 1 | Actor Loss: -0.11532668769359589 | Critic Loss: 0.16473332047462463 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.67s, TFLOPs: 4.99, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.19s, Per-token Latency 28.07 ms, TFLOPs: 0.95, BW: 491.48 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.49s, TFLOPs: 16.67\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.76953125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.55s |Gather latency=0.69s (5.49%) |Generate time=6.50s (51.75%) |Training time=3.02s (24.04%) |Others=3.04 (24.20%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.60\n",
      "Epoch: 0 | Step: 13 | PPO Epoch: 1 | Actor Loss: 0.22110921144485474 | Critic Loss: 0.09326496720314026 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.80s, TFLOPs: 4.93, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.20s, Per-token Latency 28.12 ms, TFLOPs: 0.95, BW: 490.71 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.60s, TFLOPs: 15.95\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.8046875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.67s |Gather latency=0.72s (5.64%) |Generate time=6.48s (51.14%) |Training time=3.10s (24.46%) |Others=3.09 (24.40%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.60\n",
      "Epoch: 0 | Step: 14 | PPO Epoch: 1 | Actor Loss: 0.19752414524555206 | Critic Loss: 0.1467781513929367 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.68s, TFLOPs: 4.99, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.15s, Per-token Latency 27.95 ms, TFLOPs: 0.96, BW: 493.66 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.52s, TFLOPs: 16.43\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.130859375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.58s |Gather latency=0.66s (5.23%) |Generate time=6.50s (51.62%) |Training time=3.02s (24.04%) |Others=3.06 (24.34%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.60\n",
      "Epoch: 0 | Step: 15 | PPO Epoch: 1 | Actor Loss: 0.2066921442747116 | Critic Loss: 0.08058866113424301 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.80s, TFLOPs: 4.93, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.21s, Per-token Latency 28.18 ms, TFLOPs: 0.95, BW: 489.63 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.59s, TFLOPs: 16.02\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.87109375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.65s |Gather latency=0.74s (5.89%) |Generate time=6.47s (51.10%) |Training time=3.09s (24.41%) |Others=3.10 (24.49%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.60\n",
      "Epoch: 0 | Step: 16 | PPO Epoch: 1 | Actor Loss: 0.32620859146118164 | Critic Loss: 0.08099250495433807 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.72s, TFLOPs: 4.97, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.20s, Per-token Latency 28.12 ms, TFLOPs: 0.95, BW: 490.58 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.52s, TFLOPs: 16.45\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.759765625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.59s |Gather latency=0.73s (5.82%) |Generate time=6.47s (51.35%) |Training time=3.02s (23.96%) |Others=3.11 (24.68%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.61\n",
      "[2023-10-05 05:02:04,424] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
      "Epoch: 0 | Step: 17 | PPO Epoch: 1 | Actor Loss: 0.5062612295150757 | Critic Loss: 0.1660643219947815 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.68s, TFLOPs: 4.99, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.21s, Per-token Latency 28.16 ms, TFLOPs: 0.95, BW: 489.94 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.47s, TFLOPs: 16.77\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.015625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.58s |Gather latency=0.74s (5.87%) |Generate time=6.47s (51.44%) |Training time=2.99s (23.76%) |Others=3.12 (24.80%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 18 | PPO Epoch: 1 | Actor Loss: 0.27817291021347046 | Critic Loss: 0.06772293895483017 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.69s, TFLOPs: 4.98, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.18s, Per-token Latency 28.04 ms, TFLOPs: 0.95, BW: 492.01 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.51s, TFLOPs: 16.48\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.669921875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.64s |Gather latency=0.66s (5.23%) |Generate time=6.52s (51.54%) |Training time=3.04s (24.01%) |Others=3.09 (24.45%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.61\n",
      "[2023-10-05 05:02:29,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=8, lr=[1.158e-06, 6e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2023-10-05 05:02:29,884] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=6.153714962156545, CurrSamplesPerSec=6.252130101660555, MemAllocated=8.17GB, MaxMemAllocated=26.53GB\n",
      "[2023-10-05 05:02:31,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=8, lr=[6.000000000000001e-07, 6e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "Epoch: 0 | Step: 19 | PPO Epoch: 1 | Actor Loss: 0.19121107459068298 | Critic Loss: 0.07456808537244797 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.63s, TFLOPs: 5.01, Samples/sec: 0.83, Time/seq 1.20s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.13s, Per-token Latency 27.86 ms, TFLOPs: 0.96, BW: 495.18 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.50s, TFLOPs: 16.58\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.01171875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.82s |Gather latency=0.66s (5.17%) |Generate time=6.47s (50.45%) |Training time=3.01s (23.47%) |Others=3.34 (26.08%)|CurSamplesPerSec=0.62 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 20 | PPO Epoch: 1 | Actor Loss: -0.009368296712636948 | Critic Loss: 0.041056398302316666 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.69s, TFLOPs: 4.98, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.18s, Per-token Latency 28.04 ms, TFLOPs: 0.95, BW: 492.05 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.51s, TFLOPs: 16.50\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.708984375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.59s |Gather latency=0.70s (5.56%) |Generate time=6.48s (51.45%) |Training time=3.02s (23.95%) |Others=3.10 (24.59%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 21 | PPO Epoch: 1 | Actor Loss: -0.06985516846179962 | Critic Loss: 0.024211758747696877 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.70s, TFLOPs: 4.97, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.18s, Per-token Latency 28.06 ms, TFLOPs: 0.95, BW: 491.73 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.52s, TFLOPs: 16.44\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.931640625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.59s |Gather latency=0.66s (5.24%) |Generate time=6.52s (51.78%) |Training time=3.03s (24.07%) |Others=3.04 (24.15%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 22 | PPO Epoch: 1 | Actor Loss: -0.13632382452487946 | Critic Loss: 0.05568423867225647 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.72s, TFLOPs: 4.97, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.20s, Per-token Latency 28.11 ms, TFLOPs: 0.95, BW: 490.74 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.52s, TFLOPs: 16.42\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.908203125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.62s |Gather latency=0.71s (5.66%) |Generate time=6.48s (51.35%) |Training time=3.03s (23.99%) |Others=3.11 (24.65%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.61\n",
      "[2023-10-05 05:03:20,320] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
      "[2023-10-05 05:03:21,508] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
      "Epoch: 0 | Step: 23 | PPO Epoch: 1 | Actor Loss: -0.14109200239181519 | Critic Loss: 0.035897452384233475 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.69s, TFLOPs: 4.98, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.21s, Per-token Latency 28.15 ms, TFLOPs: 0.95, BW: 490.11 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.48s, TFLOPs: 16.69\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.408203125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.63s |Gather latency=0.74s (5.83%) |Generate time=6.47s (51.23%) |Training time=3.05s (24.18%) |Others=3.10 (24.59%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 24 | PPO Epoch: 1 | Actor Loss: -0.38215118646621704 | Critic Loss: 0.09332225471735 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.76s, TFLOPs: 4.95, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.19s, Per-token Latency 28.09 ms, TFLOPs: 0.95, BW: 491.19 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.57s, TFLOPs: 16.15\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.7421875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.69s |Gather latency=0.72s (5.68%) |Generate time=6.47s (50.96%) |Training time=3.10s (24.39%) |Others=3.13 (24.65%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.61\n",
      "[2023-10-05 05:03:45,604] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128\n",
      "[2023-10-05 05:03:46,791] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128\n",
      "Epoch: 0 | Step: 25 | PPO Epoch: 1 | Actor Loss: -0.12153036147356033 | Critic Loss: 0.015035899356007576 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.67s, TFLOPs: 4.99, Samples/sec: 0.83, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.19s, Per-token Latency 28.09 ms, TFLOPs: 0.95, BW: 491.14 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.48s, TFLOPs: 16.69\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.67578125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.59s |Gather latency=0.73s (5.77%) |Generate time=6.46s (51.34%) |Training time=3.02s (24.00%) |Others=3.10 (24.66%)|CurSamplesPerSec=0.64 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 26 | PPO Epoch: 1 | Actor Loss: -0.3294372856616974 | Critic Loss: 0.06078525260090828 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.72s, TFLOPs: 4.97, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.15s, Per-token Latency 27.95 ms, TFLOPs: 0.96, BW: 493.71 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.56s, TFLOPs: 16.16\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.505859375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.65s |Gather latency=0.68s (5.40%) |Generate time=6.47s (51.12%) |Training time=3.09s (24.40%) |Others=3.10 (24.48%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 27 | PPO Epoch: 1 | Actor Loss: 0.032051894813776016 | Critic Loss: 0.018598901107907295 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.73s, TFLOPs: 4.96, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.19s, Per-token Latency 28.07 ms, TFLOPs: 0.95, BW: 491.46 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.54s, TFLOPs: 16.31\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.716796875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.64s |Gather latency=0.72s (5.68%) |Generate time=6.47s (51.18%) |Training time=3.05s (24.17%) |Others=3.12 (24.66%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 28 | PPO Epoch: 1 | Actor Loss: 0.060074739158153534 | Critic Loss: 0.008464260026812553 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.71s, TFLOPs: 4.97, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.18s, Per-token Latency 28.04 ms, TFLOPs: 0.95, BW: 492.11 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.54s, TFLOPs: 16.34\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.638671875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.61s |Gather latency=0.70s (5.52%) |Generate time=6.48s (51.40%) |Training time=3.05s (24.20%) |Others=3.08 (24.39%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.62\n",
      "[2023-10-05 05:04:36,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=10, lr=[1.93e-06, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2023-10-05 05:04:36,115] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=6.158623011000146, CurrSamplesPerSec=6.1338953957073885, MemAllocated=8.17GB, MaxMemAllocated=26.53GB\n",
      "[2023-10-05 05:04:37,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=10, lr=[1.0000000000000002e-06, 0.0001], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "Epoch: 0 | Step: 29 | PPO Epoch: 1 | Actor Loss: -0.16548645496368408 | Critic Loss: 0.0509062185883522 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.73s, TFLOPs: 4.96, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.19s, Per-token Latency 28.09 ms, TFLOPs: 0.95, BW: 491.25 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.54s, TFLOPs: 16.33\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.876953125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.64s |Gather latency=0.70s (5.57%) |Generate time=6.48s (51.29%) |Training time=3.05s (24.16%) |Others=3.10 (24.55%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 30 | PPO Epoch: 1 | Actor Loss: 0.03441771864891052 | Critic Loss: 0.0190921388566494 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.77s, TFLOPs: 4.94, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.24s, Per-token Latency 28.27 ms, TFLOPs: 0.94, BW: 488.07 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.53s, TFLOPs: 16.39\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.939453125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.66s |Gather latency=0.75s (5.96%) |Generate time=6.48s (51.19%) |Training time=3.05s (24.07%) |Others=3.13 (24.74%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 31 | PPO Epoch: 1 | Actor Loss: -0.0561893992125988 | Critic Loss: 0.009070091880857944 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.78s, TFLOPs: 4.94, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.18s, Per-token Latency 28.04 ms, TFLOPs: 0.95, BW: 492.10 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.60s, TFLOPs: 15.91\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.958984375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.67s |Gather latency=0.70s (5.52%) |Generate time=6.48s (51.13%) |Training time=3.06s (24.14%) |Others=3.13 (24.73%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 32 | PPO Epoch: 1 | Actor Loss: 0.12046398222446442 | Critic Loss: 0.05402868241071701 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.56s, TFLOPs: 4.57, Samples/sec: 0.76, Time/seq 1.32s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.89s, Per-token Latency 30.84 ms, TFLOPs: 0.87, BW: 447.37 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.67s, TFLOPs: 15.54\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.80859375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.61s |Gather latency=0.70s (5.16%) |Generate time=7.19s (52.85%) |Training time=3.27s (24.02%) |Others=3.15 (23.13%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 33 | PPO Epoch: 1 | Actor Loss: -0.043512433767318726 | Critic Loss: 0.006877083331346512 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.70s, TFLOPs: 4.98, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.17s, Per-token Latency 28.00 ms, TFLOPs: 0.95, BW: 492.77 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.54s, TFLOPs: 16.34\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.873046875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.61s |Gather latency=0.67s (5.27%) |Generate time=6.50s (51.55%) |Training time=3.05s (24.16%) |Others=3.06 (24.29%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 34 | PPO Epoch: 1 | Actor Loss: 0.1337203085422516 | Critic Loss: 0.018873970955610275 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.33s, TFLOPs: 4.68, Samples/sec: 0.77, Time/seq 1.29s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.69s, Per-token Latency 30.04 ms, TFLOPs: 0.89, BW: 459.24 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.63s, TFLOPs: 15.73\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.96875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.36s |Gather latency=0.67s (5.04%) |Generate time=7.02s (52.53%) |Training time=3.21s (24.02%) |Others=3.13 (23.45%)|CurSamplesPerSec=0.60 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 35 | PPO Epoch: 1 | Actor Loss: 0.07351234555244446 | Critic Loss: 0.007854906842112541 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.43s, TFLOPs: 4.63, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.80s, Per-token Latency 30.46 ms, TFLOPs: 0.88, BW: 452.98 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.64s, TFLOPs: 15.71\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.015625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.43s |Gather latency=0.68s (5.05%) |Generate time=7.12s (53.01%) |Training time=3.20s (23.85%) |Others=3.11 (23.14%)|CurSamplesPerSec=0.60 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 36 | PPO Epoch: 1 | Actor Loss: 0.1098938137292862 | Critic Loss: 0.010309161618351936 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.44s, TFLOPs: 4.63, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.80s, Per-token Latency 30.47 ms, TFLOPs: 0.88, BW: 452.87 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.64s, TFLOPs: 15.72\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.755859375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.47s |Gather latency=0.69s (5.15%) |Generate time=7.10s (52.76%) |Training time=3.20s (23.77%) |Others=3.16 (23.47%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.62\n",
      "[2023-10-05 05:06:21,307] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
      "[2023-10-05 05:06:22,530] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 128, reducing to 64\n",
      "Epoch: 0 | Step: 37 | PPO Epoch: 1 | Actor Loss: nan | Critic Loss: nan | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.40s, TFLOPs: 4.64, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.84s, Per-token Latency 30.61 ms, TFLOPs: 0.87, BW: 450.74 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.57s, TFLOPs: 16.15\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.458984375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.41s |Gather latency=0.68s (5.05%) |Generate time=7.16s (53.37%) |Training time=3.19s (23.79%) |Others=3.06 (22.85%)|CurSamplesPerSec=0.60 |AvgSamplesPerSec=0.62\n",
      "Epoch: 0 | Step: 38 | PPO Epoch: 1 | Actor Loss: 0.09310789406299591 | Critic Loss: 0.010021164081990719 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.46s, TFLOPs: 4.61, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.81s, Per-token Latency 30.51 ms, TFLOPs: 0.88, BW: 452.26 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.65s, TFLOPs: 15.61\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.826171875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.50s |Gather latency=0.71s (5.26%) |Generate time=7.10s (52.56%) |Training time=3.25s (24.08%) |Others=3.15 (23.36%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.62\n",
      "[2023-10-05 05:06:48,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=11, lr=[2.7985e-06, 0.000145], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2023-10-05 05:06:48,269] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=6.104288679962773, CurrSamplesPerSec=5.732016231335826, MemAllocated=8.17GB, MaxMemAllocated=26.53GB\n",
      "[2023-10-05 05:06:49,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=11, lr=[1.45e-06, 0.000145], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "Epoch: 0 | Step: 39 | PPO Epoch: 1 | Actor Loss: 0.012116273865103722 | Critic Loss: 0.013608353212475777 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.49s, TFLOPs: 4.60, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.81s, Per-token Latency 30.51 ms, TFLOPs: 0.88, BW: 452.23 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.68s, TFLOPs: 15.44\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.01171875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.50s |Gather latency=0.72s (5.31%) |Generate time=7.09s (52.52%) |Training time=3.25s (24.06%) |Others=3.16 (23.42%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 40 | PPO Epoch: 1 | Actor Loss: -0.05396920442581177 | Critic Loss: 0.00963212177157402 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.46s, TFLOPs: 4.62, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.83s, Per-token Latency 30.58 ms, TFLOPs: 0.87, BW: 451.15 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.63s, TFLOPs: 15.75\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.259765625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.45s |Gather latency=0.71s (5.29%) |Generate time=7.12s (52.90%) |Training time=3.20s (23.82%) |Others=3.13 (23.28%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 41 | PPO Epoch: 1 | Actor Loss: -0.13446582853794098 | Critic Loss: 0.013728713616728783 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.47s, TFLOPs: 4.61, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.83s, Per-token Latency 30.60 ms, TFLOPs: 0.87, BW: 450.93 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.63s, TFLOPs: 15.74\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.974609375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.47s |Gather latency=0.72s (5.34%) |Generate time=7.11s (52.79%) |Training time=3.21s (23.82%) |Others=3.15 (23.39%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 42 | PPO Epoch: 1 | Actor Loss: -0.18870557844638824 | Critic Loss: 0.02328687347471714 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.48s, TFLOPs: 4.61, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.85s, Per-token Latency 30.65 ms, TFLOPs: 0.87, BW: 450.19 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.64s, TFLOPs: 15.72\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.00390625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.47s |Gather latency=0.75s (5.59%) |Generate time=7.09s (52.64%) |Training time=3.21s (23.81%) |Others=3.17 (23.55%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "[2023-10-05 05:07:42,104] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32\n",
      "[2023-10-05 05:07:43,330] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 64, reducing to 32\n",
      "Epoch: 0 | Step: 43 | PPO Epoch: 1 | Actor Loss: -0.20078876614570618 | Critic Loss: 0.03278258442878723 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.39s, TFLOPs: 4.65, Samples/sec: 0.77, Time/seq 1.30s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.82s, Per-token Latency 30.56 ms, TFLOPs: 0.87, BW: 451.54 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.57s, TFLOPs: 16.15\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.697265625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.40s |Gather latency=0.72s (5.35%) |Generate time=7.10s (53.00%) |Training time=3.18s (23.69%) |Others=3.12 (23.31%)|CurSamplesPerSec=0.60 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 44 | PPO Epoch: 1 | Actor Loss: 0.1597766876220703 | Critic Loss: 0.020501896739006042 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.49s, TFLOPs: 4.60, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.83s, Per-token Latency 30.58 ms, TFLOPs: 0.87, BW: 451.14 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.66s, TFLOPs: 15.57\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.666015625\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.53s |Gather latency=0.75s (5.53%) |Generate time=7.08s (52.31%) |Training time=3.26s (24.08%) |Others=3.20 (23.61%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 45 | PPO Epoch: 1 | Actor Loss: 0.10300552845001221 | Critic Loss: 0.012490976601839066 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.48s, TFLOPs: 4.61, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.84s, Per-token Latency 30.63 ms, TFLOPs: 0.87, BW: 450.41 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.64s, TFLOPs: 15.69\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.234375\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.44s |Gather latency=0.76s (5.63%) |Generate time=7.08s (52.71%) |Training time=3.21s (23.92%) |Others=3.14 (23.36%)|CurSamplesPerSec=0.60 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 46 | PPO Epoch: 1 | Actor Loss: -0.010731235146522522 | Critic Loss: 0.006404669024050236 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.50s, TFLOPs: 4.60, Samples/sec: 0.76, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.87s, Per-token Latency 30.73 ms, TFLOPs: 0.87, BW: 449.01 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.63s, TFLOPs: 15.74\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.095703125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.51s |Gather latency=0.72s (5.35%) |Generate time=7.14s (52.88%) |Training time=3.23s (23.90%) |Others=3.14 (23.22%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 47 | PPO Epoch: 1 | Actor Loss: -0.1871560961008072 | Critic Loss: 0.03121325746178627 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.45s, TFLOPs: 4.62, Samples/sec: 0.77, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.81s, Per-token Latency 30.52 ms, TFLOPs: 0.87, BW: 451.99 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.64s, TFLOPs: 15.71\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.857421875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.50s |Gather latency=0.69s (5.08%) |Generate time=7.13s (52.79%) |Training time=3.23s (23.93%) |Others=3.14 (23.28%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 48 | PPO Epoch: 1 | Actor Loss: -0.19913536310195923 | Critic Loss: 0.025035016238689423 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 10.45s, TFLOPs: 4.62, Samples/sec: 0.77, Time/seq 1.31s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.81s, Per-token Latency 30.52 ms, TFLOPs: 0.87, BW: 451.99 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.63s, TFLOPs: 15.73\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -3.04296875\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=13.47s |Gather latency=0.71s (5.26%) |Generate time=7.10s (52.74%) |Training time=3.20s (23.78%) |Others=3.16 (23.48%)|CurSamplesPerSec=0.59 |AvgSamplesPerSec=0.61\n",
      "[2023-10-05 05:09:02,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=12, lr=[3.6670000000000002e-06, 0.00019], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2023-10-05 05:09:02,176] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=6.06894164467101, CurrSamplesPerSec=6.162462940573972, MemAllocated=8.17GB, MaxMemAllocated=26.53GB\n",
      "[2023-10-05 05:09:03,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=12, lr=[1.9000000000000002e-06, 0.00019], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "Epoch: 0 | Step: 49 | PPO Epoch: 1 | Actor Loss: 0.07326439768075943 | Critic Loss: 0.010425643995404243 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.72s, TFLOPs: 4.97, Samples/sec: 0.82, Time/seq 1.22s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.19s, Per-token Latency 28.09 ms, TFLOPs: 0.95, BW: 491.15 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.53s, TFLOPs: 16.38\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.76953125\n",
      "-------------------------------------------------------------------------------------\n",
      "|E2E latency=12.61s |Gather latency=0.70s (5.57%) |Generate time=6.49s (51.44%) |Training time=3.03s (24.05%) |Others=3.09 (24.51%)|CurSamplesPerSec=0.63 |AvgSamplesPerSec=0.61\n",
      "Epoch: 0 | Step: 50 | PPO Epoch: 1 | Actor Loss: 0.18785008788108826 | Critic Loss: 0.02280050702393055 | Unsupervised Loss: 0.0\n",
      "End-to-End => Latency: 9.72s, TFLOPs: 4.97, Samples/sec: 0.82, Time/seq 1.21s, Batch Size: 8, Total Seq. Length: 512\n",
      "Generation => Latency: 7.19s, Per-token Latency 28.09 ms, TFLOPs: 0.95, BW: 491.10 GB/sec, Answer Seq. Length: 256\n",
      "Training   => Latency: 2.52s, TFLOPs: 16.42\n",
      "Actor Model Parameters => 6.898 B, Critic Model Parameters => 6.767 B\n",
      "Average reward score: -2.923828125\n",
      "-------------------------------------------------------------------------------------\n",
      "saving model ...\n",
      "saving model ...\n",
      "saving model ...\n",
      "saving model ...saving model ...\n",
      "saving model ...saving model ...\n",
      "saving model ...\n",
      "\n",
      "2023-10-05 05:10:02 Uploading - Uploading generated training model2023-10-05 05:09:57,612 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\n",
      "2023-10-05 05:09:57,612 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\n",
      "2023-10-05 05:09:57,613 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "\n",
      "2023-10-05 05:10:34 Completed - Resource retained for reuse\n",
      "Training seconds: 1915\n",
      "Billable seconds: 1915\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({\"sft\":actor_model_path,\"reward\":critic_model_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Updating training job with name llama7b-rlhf-dschat-2023-10-05-04-38-29-271\n"
     ]
    }
   ],
   "source": [
    "sess.update_training_job(estimator.latest_training_job.job_name, resource_config={\"KeepAlivePeriodInSeconds\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
